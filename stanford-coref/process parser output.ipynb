{
 "metadata": {
  "name": "",
  "signature": "sha256:13c62d4b379fb60d0f535c444aa9b31022f72c5f9b18c5d8d9563f7d05848ba0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from pprint import pprint\n",
      "import xmltodict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = 'stanford-corenlp-full-2014-08-27/xmlout/'\n",
      "xmlfiles = [path+filename for filename in os.listdir(path) if filename.endswith('.xml')]\n",
      "txtpath = 'stanford-corenlp-full-2014-08-27/articles/'\n",
      "txtfiles = [txtpath+filename for filename in os.listdir(txtpath) if not filename.startswith('all-articles') and filename.endswith('.txt')]\n",
      "txtout = 'stanford-corenlp-full-2014-08-27/txtout/'\n",
      "outfiles = [txtout+filename for filename in os.listdir(txtpath) if not filename.startswith('all-articles') and filename.endswith('.txt')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fileindex, filepath in enumerate(xmlfiles): \n",
      "    with open(filepath, 'r') as xmlfile: \n",
      "        filedict = xmltodict.parse(xmlfile.read())\n",
      "        \n",
      "    # make sure it's a list, if there's only one coref\n",
      "    corefs =  filedict['root']['document']['coreference']['coreference']\n",
      "#     ner = filedict['root']['document']['sentences']['sentence'][0]['tokens']['token'][0] # coref index - 1\n",
      "    sentences = filedict['root']['document']['sentences']['sentence']\n",
      "\n",
      "    # extract a list of persons and their indices in each sentence\n",
      "    names = set()\n",
      "    person_locs = []\n",
      "    for sentence in sentences: \n",
      "        tokens = sentence['tokens']['token']\n",
      "        # aggregate adjacent NER person tags into full names\n",
      "        i = 0\n",
      "        people = {}\n",
      "        while i < len(tokens):\n",
      "            if tokens[i]['NER'] == 'PERSON':\n",
      "                key = i\n",
      "                person = tokens[i]['word']\n",
      "                # keep adding to this person string while the next token is also a person\n",
      "                i += 1\n",
      "                while tokens[i]['NER'] == 'PERSON':\n",
      "                    person += ' ' + tokens[i]['word']\n",
      "                    i += 1\n",
      "                people[key] = person\n",
      "                names.add(person)\n",
      "            i += 1\n",
      "        person_locs.append(people)\n",
      "\n",
      "    # sometimes it misses last name references to the same person - can we merge them?\n",
      "    # do another pass on person_locs to clean up names\n",
      "    names_to_replace = {}\n",
      "    for name in names: \n",
      "        other_names = {n for n in names if n != name}\n",
      "        # NOTE: this is a naive algorithm, it just picks the first match\n",
      "        # even though multiple people may have the same last name\n",
      "        for n in other_names: \n",
      "            if n in name:\n",
      "                names_to_replace[n] = name\n",
      "    for sent in person_locs:\n",
      "        for loc in sent:\n",
      "            if sent[loc] in names_to_replace:\n",
      "                sent[loc] = names_to_replace[sent[loc]]\n",
      "    \n",
      "    # assemble NERs that correspond to each coref chain\n",
      "    corefs_text = {}\n",
      "    corefs_ends = {}\n",
      "    for coref_chain in corefs:\n",
      "        indices = {}\n",
      "        ends = {}\n",
      "        # weird stuff happens if there's only one coref chain in an article, so back up one level\n",
      "        if coref_chain == \"mention\":\n",
      "            coref_chain = corefs\n",
      "        \n",
      "        for mention in coref_chain['mention']:\n",
      "            sent_index = int(mention['sentence']) - 1\n",
      "            word_start = int(mention['start']) - 1\n",
      "            word_end = int(mention['end']) - 1\n",
      "            if word_start in person_locs[sent_index]:\n",
      "                # does anything in this chain match an NER person?\n",
      "                # IF SO, store whole coref chain,\n",
      "                # but only the names and sentence/word indices\n",
      "                sent_indices = list({int(m['sentence'])-1 for m in coref_chain['mention']})\n",
      "                for sent in sent_indices: \n",
      "                    if sent not in indices:\n",
      "                        indices[sent] = {}\n",
      "                        ends[sent] = {}\n",
      "                    # add words\n",
      "                    words_in_sent = {int(m['start'])-1: '' \n",
      "                                     for m in coref_chain['mention'] \n",
      "                                     if m['sentence'] == str(sent+1)}\n",
      "                    indices[sent].update(words_in_sent)\n",
      "                    # add endings\n",
      "                    ends_in_sent = {int(m['start'])-1: int(m['end'])-1 \n",
      "                                    for m in coref_chain['mention'] \n",
      "                                    if m['sentence'] == str(sent+1)}\n",
      "                    ends[sent].update(ends_in_sent)\n",
      "                break\n",
      "                \n",
      "        if indices:\n",
      "            # get all NER person hits in the chain\n",
      "            # TODO: what if they don't match exactly (eg, coref only catches the last name?)\n",
      "            ners = []\n",
      "            for sent_index in indices: \n",
      "                for word in indices[sent_index]:\n",
      "                    if word in person_locs[sent_index]:\n",
      "                        ners.append(person_locs[sent_index][word])    \n",
      "            # canonical name is the longest one, not necessarily the first\n",
      "            name = \"{{\" + max(ners, key=len) + \"}}\"  \n",
      "            # braces just to make clear this was replaced by the program\n",
      "            \n",
      "            # replace matched indices with name in corefs_text\n",
      "            for sent_index in indices:\n",
      "                if sent_index not in corefs_text:\n",
      "                    corefs_text[sent_index] = {}\n",
      "                    corefs_ends[sent_index] = {}\n",
      "                for word in indices[sent_index]:\n",
      "                    corefs_text[sent_index][word] = name\n",
      "                    corefs_ends[sent_index].update(ends[sent_index])\n",
      "                \n",
      "    # get original text, and replace corefs with canonical names\n",
      "    with open(txtfiles[fileindex], 'r') as infile:\n",
      "        article = infile.read()\n",
      "    \n",
      "    # go in reverse order so the offsets don't cascade\n",
      "    sents_to_replace = sorted(corefs_text.keys())\n",
      "    sents_to_replace.reverse()\n",
      "    for i in sents_to_replace:\n",
      "        # look up the character offsets for each word\n",
      "        sentence = filedict['root']['document']['sentences']['sentence'][i]\n",
      "        tokens = sentence['tokens']['token']\n",
      "        locs = sorted(corefs_text[i].keys())\n",
      "        locs.reverse()\n",
      "        splice = [(int(tokens[loc]['CharacterOffsetBegin']), int(tokens[corefs_ends[i][loc]-1]['CharacterOffsetEnd']))\n",
      "                      for loc in locs]\n",
      "        \n",
      "        # account for overlapping coreferences\n",
      "        for index, block in enumerate(splice):\n",
      "            if index + 1 < len(splice):\n",
      "                # detect an overlap in character ranges and keep the one that starts first\n",
      "                if block[0] < splice[index + 1][1]:\n",
      "                    splice[index] = None\n",
      "\n",
      "        for index,loc in enumerate(locs):\n",
      "            # if splice is None, skip it\n",
      "            if splice[index]:\n",
      "                article = article[:splice[index][0]] + corefs_text[i][loc] + article[splice[index][1]:]\n",
      "\n",
      "#     print article\n",
      "#     print \"-\" * 30\n",
      "    \n",
      "    with open(outfiles[fileindex], 'w') as outfile:\n",
      "        outfile.write(article)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compare NLTK NER tool to Stanford output\n",
      "nltk.ne_chunk\n",
      "for fileindex, filepath in enumerate(xmlfiles): \n",
      "    with open(filepath, 'r') as xmlfile: \n",
      "        filedict = xmltodict.parse(xmlfile.read())\n",
      "        \n",
      "    sentences = filedict['root']['document']['sentences']['sentence']\n",
      "    tagged_sentences = []\n",
      "    # construct tagged token list for nltk input\n",
      "    for sentence in sentences: \n",
      "        tagged_sentence = []\n",
      "        tokens = sentence['tokens']['token']\n",
      "        print 'StanfordNLP NER:'\n",
      "        for token in tokens: \n",
      "            tagged_sentence.append((token['word'], token['POS']))\n",
      "            if token['NER'] == 'PERSON':\n",
      "                print token['word']\n",
      "        print ' '\n",
      "        print 'NLTK NER:'\n",
      "        ner = nltk.ne_chunk(tagged_sentence)\n",
      "        for subtree in ner.subtrees():\n",
      "            if subtree.node == 'PERSON':\n",
      "                print subtree\n",
      "        # tagged_sentences.append(tagged_sentence)\n",
      "        print '-' * 30\n",
      "        print ' '\n",
      "# Stanford seems to be better, at least out of the box    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}