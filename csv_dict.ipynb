{
 "metadata": {
  "name": "",
  "signature": "sha256:97a3ff3615c87fd3afd17ae717e8df051b7178a4d8093e54c111787f3e362d6b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import ast\n",
      "import csv\n",
      "import itertools\n",
      "from nltk import RegexpParser\n",
      "import pprint\n",
      "import json\n",
      "pp = pprint.PrettyPrinter(indent=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Read CSV as Dict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_dict = []\n",
      "with open('all_sentences_tagged_indexed__complete.csv') as csvfile:\n",
      "    reader = csv.DictReader(csvfile)\n",
      "    for key, row in enumerate(reader):\n",
      "        data = {}\n",
      "        data['sent_index'] = key\n",
      "        data['story_id'] = int(row['story_id'])\n",
      "        data['para_index'] = int(row['para_index'])\n",
      "        data['tagged_sent'] = ast.literal_eval(row['tagged_sent'])\n",
      "        data['quote_in_para'] = row['quote_in_para']\n",
      "        df_dict.append(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load Single Document"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEST_ARTICLE = []\n",
      "for data in df_dict:\n",
      "    if data['story_id'] == 13662:\n",
      "        TEST_ARTICLE.append(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "quote_grammar = '''\n",
      "NONQUOTE:   { <.*>+ } \n",
      "            } (<``>|<\\'\\'>) <.*>+ (<''>|<\\'\\'>) {\n",
      "            } (<``>|<\\'\\'>) <.*>+ (<''>|<\\'\\'>)? {\n",
      "            } (<``>|<\\'\\'>)? <.*>+ (<''>|<\\'\\'>) {\n",
      "\n",
      "QUOTE:      { (<``>|<\\'\\'>) <.*>+ (<''>|<\\'\\'>) }\n",
      "            { (<``>|<\\'\\'>) <.*>+ (<''>|<\\'\\'>)? }\n",
      "            { (<``>|<\\'\\'>)? <.*>+ (<''>|<\\'\\'>) }\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### FUNCTIONS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def group_dict_func(list_of_dicts):\n",
      "    keyfunc = lambda x: x['para_index']\n",
      "    groups = []\n",
      "    data = sorted(list_of_dicts, key=keyfunc)\n",
      "    for k, g in itertools.groupby(data, key=keyfunc):\n",
      "        groups.append(list(g))\n",
      "    para_dict_list = []\n",
      "    for g in groups:\n",
      "        para_dict = {}\n",
      "        para_dict['tagged_sent'] = []\n",
      "        for l in g:\n",
      "            para_dict['para_index'] = int(l['para_index'])\n",
      "            para_dict['quote_in_para'] = int(l['quote_in_para'])\n",
      "            para_dict['story_id'] = int(l['story_id'])\n",
      "            para_dict['tagged_sent'].append(l['tagged_sent'])\n",
      "        para_dict_list.append(para_dict)\n",
      "    return para_dict_list\n",
      "\n",
      "\n",
      "def find_ngrams(input_list, n):\n",
      "    return zip(*[input_list[i:] for i in range(n)])\n",
      "\n",
      "\n",
      "def parsing_function(ARTICLE_DICT, i, lookahead=0, featurenames='', getquotes=False):\n",
      "    '''features for all paragraphs'''\n",
      "    list_of_sents = ARTICLE_DICT[i+lookahead]['tagged_sent']           # take the tagged list of sentences\n",
      "    paragraph_quote_dict = {}\n",
      "    number_of_sections = 0                     # count number of quote and nonquote sections in order to add to dict\n",
      "    for key, sent in enumerate(list_of_sents):\n",
      "        tree = cp.parse(sent)\n",
      "        for subtree in tree.subtrees():\n",
      "            if subtree.label() == 'NONQUOTE' or subtree.label() == 'QUOTE':\n",
      "                paragraph_quote_dict[number_of_sections] = subtree\n",
      "                number_of_sections += 1    \n",
      "            if getquotes and subtree.label() == 'QUOTE':\n",
      "                ARTICLE_DICT[i]['quotations'].append(' '.join(s[0] for s in subtree))\n",
      "    \n",
      "                \n",
      "    for key in paragraph_quote_dict.keys():\n",
      "        if paragraph_quote_dict[key]._label == 'NONQUOTE':\n",
      "            inverse_tuple_chunk = zip(*paragraph_quote_dict[key].leaves())\n",
      "            pos_tags = inverse_tuple_chunk[1]\n",
      "            words = inverse_tuple_chunk[0]\n",
      "            pos_bigrams = find_ngrams(pos_tags, 2)\n",
      "            word_bigrams = find_ngrams(pos_tags, 2)\n",
      "\n",
      "            if 'said' in words and 'COREF' in pos_tags:\n",
      "                said_match_indices = [x for x, y in enumerate(pos_tags) if y == 'COREF']\n",
      "                matching_corefs = []\n",
      "                for index in said_match_indices:\n",
      "                    matching_corefs.append(words[index])\n",
      "                ARTICLE_DICT[i]['feature_said_coref' + featurenames] = matching_corefs\n",
      "\n",
      "            if ('COREF', u'VBD') in pos_bigrams and 'said' not in words:                    \n",
      "                pos_match_indices = [x for x, y in enumerate(pos_bigrams) if y == ('COREF', u'VBD')]\n",
      "                matching_pos_words = []\n",
      "                for index in pos_match_indices:\n",
      "                    matching_pos_words.append(words[index])\n",
      "                ARTICLE_DICT[i]['feature_coref_vbd' + featurenames] = matching_pos_words\n",
      "\n",
      "            if (u'VBD', 'COREF') in pos_bigrams and 'said' not in words:\n",
      "                pos_match_indices = [x for x, y in enumerate(pos_bigrams) if y == (u'VBD', 'COREF')]\n",
      "                matching_pos_words = []\n",
      "                for index in pos_match_indices:\n",
      "                    matching_pos_words.append(words[index+1])\n",
      "                ARTICLE_DICT[i]['feature_vbd_coref' + featurenames] = matching_pos_words\n",
      "            \n",
      "            if 'COREF' in pos_tags:\n",
      "                said_match_indices = [x for x, y in enumerate(pos_tags) if y == 'COREF']\n",
      "                matching_corefs = []\n",
      "                for index in said_match_indices:\n",
      "                    matching_corefs.append(words[index])\n",
      "                ARTICLE_DICT[i]['feature_coref_only' + featurenames] = matching_corefs\n",
      "\n",
      "def jsonify_sents(paragraph):\n",
      "    tagged_sentences_pos = [item for sublist in paragraph for item in sublist]\n",
      "    tagged_sentences = []\n",
      "    for word in tagged_sentences_pos:\n",
      "        if word[1] == 'COREF':\n",
      "            tagged_sentences.append(str(word))\n",
      "        else:\n",
      "            tagged_sentences.append(word[0])\n",
      "\n",
      "    return ' '.join(tagged_sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEST_ARTICLE = []\n",
      "for data in df_dict:\n",
      "    if data['story_id'] == 13669:\n",
      "        TEST_ARTICLE.append(data)\n",
      "        \n",
      "ARTICLE_DICT = group_dict_func(TEST_ARTICLE)       # this is what we're adding features to \n",
      "LAST_PARA_INDEX = len(ARTICLE_DICT) - 1\n",
      "\n",
      "for i, j in enumerate(ARTICLE_DICT):\n",
      "    ARTICLE_DICT[i]['quotations'] = []\n",
      "    cp = RegexpParser(quote_grammar)\n",
      "    cp_bet = RegexpParser(quote_grammar)\n",
      "    if ARTICLE_DICT[i]['quote_in_para'] == 1:                    # if there's a quote in the paragraph\n",
      "        parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=0, featurenames='', getquotes=True)\n",
      "\n",
      "        if j['para_index'] == 0:\n",
      "            '''First paragraph'''\n",
      "            if ARTICLE_DICT[i+1]['quote_in_para'] == 1:                    # if there's a quote in the paragraph\n",
      "                parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=1, featurenames='_ahead')\n",
      "\n",
      "        elif j['para_index'] == LAST_PARA_INDEX:\n",
      "            '''Last paragraph'''\n",
      "            if ARTICLE_DICT[i-1]['quote_in_para'] == 1:\n",
      "                parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=-1, featurenames='_back')\n",
      "            \n",
      "        else:\n",
      "            '''Middle paragraph'''\n",
      "            parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=1, featurenames='_ahead')\n",
      "            parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=-1, featurenames='_back')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ARTICLE_DF = pd.DataFrame(ARTICLE_DICT).fillna(0)\n",
      "#list(ARTICLE_DF.columns.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "JSON_LIST = []\n",
      "for index, row in ARTICLE_DF.iterrows():\n",
      "    JSON_DICT = {}\n",
      "    JSON_DICT['quotations'] = row.quotations\n",
      "    JSON_DICT['paragraph'] = jsonify_sents(row.tagged_sent)\n",
      "    JSON_DICT['quote_in_para'] = row.quote_in_para\n",
      "    JSON_DICT['para_index'] = row.para_index\n",
      "    JSON_DICT['story_id'] = row.story_id\n",
      "    if row.quote_in_para == 1:\n",
      "        JSON_DICT['speaker'] = 1\n",
      "        if row.feature_said_coref:\n",
      "            JSON_DICT['speaker'] = row.feature_said_coref\n",
      "        elif row.feature_vbd_coref:\n",
      "            JSON_DICT['speaker'] = row.feature_vbd_coref\n",
      "        elif row.feature_coref_vbd:\n",
      "            JSON_DICT['speaker'] = row.feature_coref_vbd\n",
      "        elif row.feature_coref_only:\n",
      "            JSON_DICT['speaker'] = row.feature_coref_only\n",
      "        elif row.feature_said_coref_back:\n",
      "            JSON_DICT['speaker'] = row.feature_said_coref_back\n",
      "        elif row.feature_coref_only_back:\n",
      "            JSON_DICT['speaker'] = row.feature_coref_only_back\n",
      "        elif row.feature_coref_vbd_back:\n",
      "            JSON_DICT['speaker'] = row.feature_coref_vbd_back\n",
      "    elif row.quote_in_para == 0:\n",
      "        JSON_DICT['speaker'] = None\n",
      "    JSON_LIST.append(JSON_DICT)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#JSON_LIST"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# write to JSON"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('data.json', 'w') as outfile:\n",
      "    json.dump(JSON_LIST, outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Examples in nonquote section\n",
      "\n",
      "### Said, coref + Coref, said\n",
      "```\n",
      "[(u'Jason Overman', 'COREF'), (u'said', u'VBD'), (u'.', u'.')]\n",
      "\n",
      "[(u'said', u'VBD'), (u'Bonnie Trinclisti', 'COREF'), (u'.', u'.')]\n",
      "\n",
      "[(u'said', u'VBD'), (u'Peter Straus', 'COREF'), (u',', u','), (u'Peter Straus', 'COREF'), (u'.', u'.')]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Said, with stuff in between coref\n",
      "```\n",
      "[(u'said', u'VBD'), (u'California', u'NNP'), (u'State', u'NNP'), (u'Parks', u'NNP'), (u'Foundation', u'NNP'), (u'president', u'NN'), (u'Elizabeth', u'NNP'), (u'Goldstein', u'NNP'), (u'.', u'.')]\n",
      "\n",
      "[(u'said', u'VBD'), (u'LandPaths', u'NNP'), (u\"'\", u'POS'), (u'executive', u'JJ'), (u'director', u'NN'), (u'Craig', u'NNP'), (u'Anderson', u'NNP'), (u'.', u'.')]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Said, non-person speaking\n",
      "```\n",
      "[(u'Reducing', u'VBG'), (u'the', u'DT'), (u'number', u'NN'), (u'of', u'IN'), (u'peace', u'NN'), (u'officers', u'NNS'), (u'could', u'MD'), (u'reduce', u'VB'), (u'state', u'NN'), (u'parks', u'NNS'), (u\"'\", u'POS'), (u'expenses', u'NNS'), (u'by', u'IN'), (u'at', u'IN'), (u'least', u'JJS'), (u'a', u'DT'), (u'few', u'JJ'), (u'million', u'CD'), (u'dollars', u'NNS'), (u',', u','), (u'the', u'DT'), (u'report', u'NN'), (u'said', u'VBD'), (u'.', u'.')]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Coref, VBD (e.g. \"added\")\n",
      "```\n",
      "[(u'The', u'DT'), (u'steps', u'NNS'), (u'Muni', 'COREF'), (u'is', u'VBZ'), (u'taking', u'VBG'), (u',', u','), (u'Ed Reiskin', 'COREF'), (u'added', u'VBD'), (u',', u',')]\n",
      "```"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}