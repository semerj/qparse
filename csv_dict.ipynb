{
 "metadata": {
  "name": "",
  "signature": "sha256:32b19e539f95a3544a87a4af5a5a4ac2459f63505f035af394cb7b2e0c6bcbd3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import ast\n",
      "import csv\n",
      "import itertools\n",
      "from nltk import RegexpParser\n",
      "import pprint\n",
      "import json\n",
      "from unidecode import unidecode\n",
      "pp = pprint.PrettyPrinter(indent=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Read CSV as Dict"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_dict = []\n",
      "with open('all_sentences_tagged_indexed__complete.csv') as csvfile:\n",
      "    reader = csv.DictReader(csvfile)\n",
      "    for key, row in enumerate(reader):\n",
      "        data = {}\n",
      "        data['sent_index'] = key\n",
      "        data['story_id'] = int(row['story_id'])\n",
      "        data['para_index'] = int(row['para_index'])\n",
      "        data['tagged_sent'] = ast.literal_eval(row['tagged_sent'])\n",
      "        data['quote_in_para'] = row['quote_in_para']\n",
      "        df_dict.append(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load Single Document"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "quote_grammar = '''\n",
      "NONQUOTE:   { <.*>+ } \n",
      "            } <``> <.*>+ (<''>|<\\'\\'>) {\n",
      "            } <``> <.*>+ (<''>|<\\'\\'>)? {\n",
      "            } <``>? <.*>+ (<''>|<\\'\\'>) {\n",
      "\n",
      "QUOTE:      { <``> <.*>+ (<''>|<\\'\\'>) }\n",
      "            { <``> <.*>+ (<''>|<\\'\\'>)? }\n",
      "            { <``>? <.*>+ (<''>|<\\'\\'>) }\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### FUNCTIONS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_ngrams(input_list, n):\n",
      "    return zip(*[input_list[i:] for i in range(n)])\n",
      "\n",
      "\n",
      "def group_dict_func(list_of_dicts):\n",
      "    keyfunc = lambda x: x['para_index']\n",
      "    groups = []\n",
      "    data = sorted(list_of_dicts, key=keyfunc)\n",
      "    for k, g in itertools.groupby(data, key=keyfunc):\n",
      "        groups.append(list(g))\n",
      "    para_dict_list = []\n",
      "    for g in groups:\n",
      "        para_dict = {}\n",
      "        para_dict['tagged_sent'] = []\n",
      "        for l in g:\n",
      "            para_dict['para_index'] = int(l['para_index'])\n",
      "            para_dict['quote_in_para'] = int(l['quote_in_para'])\n",
      "            para_dict['story_id'] = int(l['story_id'])\n",
      "            para_dict['tagged_sent'].append(l['tagged_sent'])\n",
      "        para_dict_list.append(para_dict)\n",
      "    return para_dict_list\n",
      "\n",
      "\n",
      "def parsing_function(ARTICLE_DICT, i, lookahead=0, featurenames='', getquotes=False):\n",
      "    cp = RegexpParser(quote_grammar)\n",
      "    list_of_sents = ARTICLE_DICT[i+lookahead]['tagged_sent']\n",
      "    paragraph_quote_dict = {}\n",
      "    number_of_sections = 0\n",
      "    for key, sent in enumerate(list_of_sents):\n",
      "        tree = cp.parse(sent)\n",
      "        for subtree in tree.subtrees():\n",
      "            if subtree.label() == 'NONQUOTE' or subtree.label() == 'QUOTE':\n",
      "                paragraph_quote_dict[number_of_sections] = subtree\n",
      "                number_of_sections += 1    \n",
      "            if getquotes and subtree.label() == 'QUOTE':\n",
      "                ARTICLE_DICT[i]['quotations'].append(' '.join(s[0] for s in subtree))\n",
      "    \n",
      "                \n",
      "    for key in paragraph_quote_dict.keys():\n",
      "        if paragraph_quote_dict[key]._label == 'NONQUOTE':\n",
      "            inverse_tuple_chunk = zip(*paragraph_quote_dict[key].leaves())\n",
      "            pos_tags = inverse_tuple_chunk[1]\n",
      "            words = inverse_tuple_chunk[0]\n",
      "            pos_bigrams = find_ngrams(pos_tags, 2)\n",
      "            word_bigrams = find_ngrams(pos_tags, 2)\n",
      "\n",
      "            if 'said' in words and 'COREF' in pos_tags:\n",
      "                said_match_indices = [x for x, y in enumerate(pos_tags) if y == 'COREF']\n",
      "                matching_corefs = []\n",
      "                for index in said_match_indices:\n",
      "                    matching_corefs.append(words[index])\n",
      "                ARTICLE_DICT[i]['feature_said_coref' + featurenames] = matching_corefs\n",
      "\n",
      "            if ('COREF', u'VBD') in pos_bigrams and 'said' not in words:                    \n",
      "                pos_match_indices = [x for x, y in enumerate(pos_bigrams) if y == ('COREF', u'VBD')]\n",
      "                matching_pos_words = []\n",
      "                for index in pos_match_indices:\n",
      "                    matching_pos_words.append(words[index])\n",
      "                ARTICLE_DICT[i]['feature_coref_vbd' + featurenames] = matching_pos_words\n",
      "\n",
      "            if (u'VBD', 'COREF') in pos_bigrams and 'said' not in words:\n",
      "                pos_match_indices = [x for x, y in enumerate(pos_bigrams) if y == (u'VBD', 'COREF')]\n",
      "                matching_pos_words = []\n",
      "                for index in pos_match_indices:\n",
      "                    matching_pos_words.append(words[index+1])\n",
      "                ARTICLE_DICT[i]['feature_vbd_coref' + featurenames] = matching_pos_words\n",
      "            \n",
      "            if 'COREF' in pos_tags:\n",
      "                said_match_indices = [x for x, y in enumerate(pos_tags) if y == 'COREF']\n",
      "                matching_corefs = []\n",
      "                for index in said_match_indices:\n",
      "                    matching_corefs.append(words[index])\n",
      "                ARTICLE_DICT[i]['feature_coref_only' + featurenames] = matching_corefs\n",
      "\n",
      "            if (u'NNP', u'NNP') in pos_tags:\n",
      "                said_match_indices = [x for x, y in enumerate(pos_tags) if y == (u'NNP', u'NNP')]\n",
      "                matching_corefs = []\n",
      "                for index in said_match_indices:\n",
      "                    matching_corefs.append(words[index])\n",
      "                ARTICLE_DICT[i]['feature_nnp_nnp' + featurenames] = matching_corefs\n",
      "                \n",
      "    return ARTICLE_DICT\n",
      "\n",
      "def jsonify_sents(paragraph):\n",
      "    tagged_sentences_pos = [item for sublist in paragraph for item in sublist]\n",
      "    tagged_sentences = []\n",
      "    for word in tagged_sentences_pos:\n",
      "        if word[1] == 'COREF':\n",
      "            tagged_sentences.append('< {}:{} >'.format(unidecode(word[0]), 'PERSON'))\n",
      "        else:\n",
      "            tagged_sentences.append(word[0])\n",
      "\n",
      "    return ' '.join(tagged_sentences)\n",
      "\n",
      "\n",
      "def group_articles(list_of_dicts):\n",
      "    groups = []\n",
      "    keyfunc = lambda x: x['story_id']\n",
      "    data = sorted(list_of_dicts, key=keyfunc)\n",
      "    for k, g in itertools.groupby(data, key=keyfunc):\n",
      "        groups.append(list(g))\n",
      "    return groups\n",
      "\n",
      "\n",
      "def algorithm(ARTICLE):\n",
      "    ARTICLE_DICT = group_dict_func(ARTICLE)\n",
      "    LAST_PARA_INDEX = len(ARTICLE_DICT) - 1\n",
      "    cp = RegexpParser(quote_grammar)\n",
      "    for i, j in enumerate(ARTICLE_DICT):\n",
      "        ARTICLE_DICT[i]['quotations'] = []\n",
      "        if ARTICLE_DICT[i]['quote_in_para'] == 1:\n",
      "            parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=0, featurenames='', getquotes=True)\n",
      "\n",
      "            if j['para_index'] == 0:\n",
      "                '''First paragraph'''\n",
      "                if ARTICLE_DICT[i+1]['quote_in_para'] == 1:\n",
      "                    parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=1, featurenames='_ahead')\n",
      "\n",
      "            elif j['para_index'] == LAST_PARA_INDEX:\n",
      "                '''Last paragraph'''\n",
      "                if ARTICLE_DICT[i-1]['quote_in_para'] == 1:\n",
      "                    parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=-1, featurenames='_back')\n",
      "\n",
      "            else:\n",
      "                '''Middle paragraph'''\n",
      "                #parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=1, featurenames='_ahead')\n",
      "                parsing_function(ARTICLE_DICT=ARTICLE_DICT, i=i, lookahead=-1, featurenames='_back')\n",
      "\n",
      "    ARTICLE_DF = pd.DataFrame(ARTICLE_DICT).fillna(0)\n",
      "\n",
      "    JSON_LIST = []\n",
      "    column_names = list(ARTICLE_DF.columns.values)\n",
      "    for index, row in ARTICLE_DF.iterrows():\n",
      "        JSON_DICT = {}\n",
      "        JSON_DICT['quotations'] = row.quotations\n",
      "        JSON_DICT['paragraph'] = jsonify_sents(row.tagged_sent)\n",
      "        JSON_DICT['quote_in_para'] = row.quote_in_para\n",
      "        JSON_DICT['para_index'] = row.para_index\n",
      "        JSON_DICT['story_id'] = row.story_id\n",
      "        if row.quote_in_para == 1:\n",
      "            if 'feature_said_coref' in column_names and row.feature_said_coref:\n",
      "                JSON_DICT['speaker'] = row.feature_said_coref\n",
      "            elif 'feature_vbd_coref' in column_names and row.feature_vbd_coref:\n",
      "                JSON_DICT['speaker'] = row.feature_vbd_coref\n",
      "            elif 'feature_coref_vbd' in column_names and row.feature_coref_vbd:\n",
      "                JSON_DICT['speaker'] = row.feature_coref_vbd\n",
      "            elif 'feature_coref_only' in column_names and row.feature_coref_only:\n",
      "                JSON_DICT['speaker'] = row.feature_coref_only\n",
      "            elif 'feature_said_coref_back' in column_names and row.feature_said_coref_back:\n",
      "                JSON_DICT['speaker'] = row.feature_said_coref_back\n",
      "            elif 'feature_coref_only_back' in column_names and row.feature_coref_only_back:\n",
      "                JSON_DICT['speaker'] = row.feature_coref_only_back\n",
      "            elif 'feature_coref_vbd_back' in column_names and row.feature_coref_vbd_back:\n",
      "                JSON_DICT['speaker'] = row.feature_coref_vbd_back\n",
      "            elif 'feature_vbd_coref_back' in column_names and row.feature_vbd_coref_back:\n",
      "                JSON_DICT['speaker'] = row.feature_coref_vbd_back\n",
      "            elif 'feature_nnp_nnp' in column_names and row.feature_nnp_nnp:\n",
      "                JSON_DICT['speaker'] = row.feature_nnp_nnp\n",
      "            elif 'feature_nnp_nnp_back' in column_names and row.feature_nnp_nnp_back:\n",
      "                JSON_DICT['speaker'] = row.feature_nnp_nnp_back   \n",
      "            else:\n",
      "                JSON_DICT['speaker'] = ['Robert J. Glushko']\n",
      "        elif row.quote_in_para == 0:\n",
      "            JSON_DICT['speaker'] = None\n",
      "        JSON_LIST.append(JSON_DICT)\n",
      "    return JSON_LIST"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Write all Citizen-Quotes articles to JSON"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "citizen_quotes_groups = group_articles(df_dict)\n",
      "\n",
      "JSON_DATA = []\n",
      "for article in citizen_quotes_groups:\n",
      "    JSON_DATA += algorithm(article)\n",
      "    \n",
      "with open('citizen_quotes.json', 'w') as outfile:\n",
      "    json.dump(JSON_DATA, outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Examples in nonquote section\n",
      "\n",
      "### Said, coref + Coref, said\n",
      "```\n",
      "[(u'Jason Overman', 'COREF'), (u'said', u'VBD'), (u'.', u'.')]\n",
      "\n",
      "[(u'said', u'VBD'), (u'Bonnie Trinclisti', 'COREF'), (u'.', u'.')]\n",
      "\n",
      "[(u'said', u'VBD'), (u'Peter Straus', 'COREF'), (u',', u','), (u'Peter Straus', 'COREF'), (u'.', u'.')]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Said, with stuff in between coref\n",
      "```\n",
      "[(u'said', u'VBD'), (u'California', u'NNP'), (u'State', u'NNP'), (u'Parks', u'NNP'), (u'Foundation', u'NNP'), (u'president', u'NN'), (u'Elizabeth', u'NNP'), (u'Goldstein', u'NNP'), (u'.', u'.')]\n",
      "\n",
      "[(u'said', u'VBD'), (u'LandPaths', u'NNP'), (u\"'\", u'POS'), (u'executive', u'JJ'), (u'director', u'NN'), (u'Craig', u'NNP'), (u'Anderson', u'NNP'), (u'.', u'.')]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Said, non-person speaking\n",
      "```\n",
      "[(u'Reducing', u'VBG'), (u'the', u'DT'), (u'number', u'NN'), (u'of', u'IN'), (u'peace', u'NN'), (u'officers', u'NNS'), (u'could', u'MD'), (u'reduce', u'VB'), (u'state', u'NN'), (u'parks', u'NNS'), (u\"'\", u'POS'), (u'expenses', u'NNS'), (u'by', u'IN'), (u'at', u'IN'), (u'least', u'JJS'), (u'a', u'DT'), (u'few', u'JJ'), (u'million', u'CD'), (u'dollars', u'NNS'), (u',', u','), (u'the', u'DT'), (u'report', u'NN'), (u'said', u'VBD'), (u'.', u'.')]\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Coref, VBD (e.g. \"added\")\n",
      "```\n",
      "[(u'The', u'DT'), (u'steps', u'NNS'), (u'Muni', 'COREF'), (u'is', u'VBZ'), (u'taking', u'VBG'), (u',', u','), (u'Ed Reiskin', 'COREF'), (u'added', u'VBD'), (u',', u',')]\n",
      "```"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}