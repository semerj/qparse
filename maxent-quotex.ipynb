{
 "metadata": {
  "name": "",
  "signature": "sha256:77194be4194df7305d9233e0d7db79913e023781fdb39a7d7fe52efe3e825758"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# MaxEnt Model for Quotation Identification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import re\n",
      "from collections import Counter\n",
      "from __future__ import division\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn import metrics\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from nltk.stem.porter import PorterStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "numpy.dtype has the wrong size, try recompiling\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "numpy.dtype has the wrong size, try recompiling",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-369a6374660f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ian/.virtualenvs/citizen-quotes/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtslib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ian/GitHub/qparse/pandas/src/numpy.pxd\u001b[0m in \u001b[0;36minit pandas.hashtable (pandas/hashtable.c:22984)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: numpy.dtype has the wrong size, try recompiling"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Import Tagged Data\n",
      "Find which table contains the tagged quotes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn = sqlite3.connect('quotex')\n",
      "c = conn.cursor()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tables = c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
      "for t in tables:\n",
      "    print t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'auth_permission',)\n",
        "(u'auth_group_permissions',)\n",
        "(u'auth_group',)\n",
        "(u'auth_user_user_permissions',)\n",
        "(u'auth_user_groups',)\n",
        "(u'auth_user',)\n",
        "(u'django_content_type',)\n",
        "(u'django_session',)\n",
        "(u'django_site',)\n",
        "(u'django_admin_log',)\n",
        "(u'content_story',)\n",
        "(u'content_paragraph_sources',)\n",
        "(u'content_paragraph',)\n",
        "(u'content_source',)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content_paragraph = c.execute(\"SELECT * FROM content_paragraph;\")\n",
      "sqlite_data = []\n",
      "for c in content_paragraph:\n",
      "    sqlite_data.append(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "300 labeled paragraphs: 88 w/ quotes and 212 w/o quotes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Counter([row[3] for row in sqlite_data])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "Counter({0: 1335, 1: 578})"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "loop through sqlite_data and only pull out tagged data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_data = []\n",
      "for k, v in enumerate(sqlite_data):\n",
      "    if v[3] in [0, 1]:\n",
      "        tagged_data.append(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Citizen Quotes Features\n",
      "features/helper functions from citizen-quotes necessary for maxent model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PUNCTUATION_TO_REMOVE = ['.', ',', '!', '?']\n",
      "ATTRIBUTION_WORDS_STEMMED = ['said', 'say', 'call', 'accus', 'tell', 'told', 'report', 'assur']\n",
      "PRONOUNS = ['he', 'she']\n",
      "\n",
      "########## HELPER FUNCTIONS ##########\n",
      "\n",
      "def bracketed_find(s, start, end, startat=0):\n",
      "    \"\"\"\n",
      "    Function to find content in between two words or characters without regex.\n",
      "    From http://stackoverflow.com/questions/1116172/finding-content-between-two-words-withou-regex-beautifulsoup-lxml-etc\n",
      "    \"\"\"\n",
      "    startloc=s.find(start, startat)\n",
      "    if startloc==-1:\n",
      "        return []\n",
      "    endloc=s.find(end, startloc+len(start))\n",
      "    if endloc == -1:\n",
      "        return [s[startloc+len(start):]]\n",
      "    return [s[startloc+len(start):endloc]] + bracketed_find(s, start, end, endloc+len(end))\n",
      "\n",
      "def get_words_outside_quotes(words, n=5):\n",
      "    \"\"\"\n",
      "    Function to get words within n characters of quote marks.\n",
      "    \"\"\"\n",
      "    quote_indices = [m.start() for m in re.finditer('\"', words)]\n",
      "    for i in range(len(quote_indices)):\n",
      "        if i % 2 != 0:\n",
      "            index = quote_indices[i] + 1\n",
      "            try:\n",
      "                next = quote_indices[i+1]\n",
      "            except IndexError:\n",
      "                next = len(words)\n",
      "            return words[index:next].strip().split()[:5]\n",
      "\n",
      "def clean_text(words):\n",
      "    \"\"\"\n",
      "    Function to clean input text by removing select punctuation and stopwords\n",
      "    and stemming with a Porter stemmer.\n",
      "    \"\"\"\n",
      "    for p in PUNCTUATION_TO_REMOVE:\n",
      "        words = words.replace(p, '')\n",
      "    stopwords = nltk.corpus.stopwords.words('english')\n",
      "    return ' '.join([PorterStemmer().stem_word(w) for w in words.split() if w not in stopwords])\n",
      "\n",
      "########## ACTIVE FEATURES ##########\n",
      "\n",
      "def contains_quotes(words):\n",
      "    \"\"\"\n",
      "    Returns true if the input string contains quote marks. Word of warning:\n",
      "    be sure unicode smart quotes are swapped out for ascii dumb quotes or \n",
      "    this won't catch them. Same goes for any other feature that looks for\n",
      "    quotes.\n",
      "    \"\"\"\n",
      "    contains_quotes = 0\n",
      "    if words.find('\"') > -1:\n",
      "        contains_quotes = 1\n",
      "    return contains_quotes\n",
      "\n",
      "def first_quote_index(words):\n",
      "    \"\"\"\n",
      "    Returns the index of the first quote mark in the input string. Rounds to\n",
      "    the nearest 10th position. So a quote that appears at position 5 in a string\n",
      "    will actually be output as 10 from this function.\n",
      "\n",
      "    The reason is because NLTK's particular maxent classifier doesn't deal well \n",
      "    with continuous variables. Grouping them as every 10th position effectively\n",
      "    makes the data categorical (the 10s, 20s, 30s, etc.)\n",
      "    \"\"\"\n",
      "    return words.find('\"')\n",
      "    #return round(words.find('\"'), -1)\n",
      "\n",
      "def last_word(words):\n",
      "    '''\n",
      "    Returns the last word in the input string. Useful because a lot of quote grafs\n",
      "    end with something like \"Smith said.\"\n",
      "    '''\n",
      "    words = clean_text(words)\n",
      "    if len(words.split()) > 0:\n",
      "        return words.split()[-1]\n",
      "    return False\n",
      "\n",
      "def said_near_source(words):\n",
      "    '''\n",
      "    Janky set of regexes that return true if the word said appears within five words\n",
      "    of a pronoun or capitalized (proper) noun. This should be rewritten for about a\n",
      "    million different reasons, but it works fine for demo purposes, so ....\n",
      "    '''\n",
      "    words = clean_text(words)\n",
      "    said_near_source = 0\n",
      "    if len(re.findall(r'\\b(he|she|[A-Z][a-z]+)\\W+(?:\\w+\\W+){0,5}(said|added|says)\\b', words)) > 0 \\\n",
      "        or len(re.findall(r'\\b(said|added|says){0,5}(he|she|[A-Z][a-z]+)\\W+(?:\\w+\\W+)\\b', words)) > 0:\n",
      "        said_near_source = 1\n",
      "    return said_near_source\n",
      "\n",
      "def num_words_between_quotes(words):\n",
      "    '''\n",
      "    Uses the helper functions above to count the number of words in the input\n",
      "    text that fall between quote marks. Returns that number rounded to the nearest\n",
      "    5 words, again to make the continuous data more categorical.\n",
      "    '''\n",
      "    num_words_between_quotes = 0\n",
      "    lots_of_words_between_quotes = False\n",
      "    for q in bracketed_find(words, '\"', '\"'):\n",
      "        num_words_between_quotes += len(q.split())\n",
      "    return round(num_words_between_quotes, -1) / 2\n",
      "\n",
      "def words_near_quotes(words):\n",
      "    '''\n",
      "    Creates features out of the 5 words (by default) that appear nearby but outside\n",
      "    the quote marks in the input text.\n",
      "    '''\n",
      "    words = clean_text(words)\n",
      "    if get_words_outside_quotes(words):\n",
      "        for word in get_words_outside_quotes(words):\n",
      "            yield word\n",
      "\n",
      "########## INACTIVE FEATURES ##########\n",
      "\n",
      "def first_word(words):\n",
      "    '''\n",
      "    Returns the first word in the input string.\n",
      "    '''\n",
      "    words = clean_text(words)\n",
      "    if len(words.split()) > 0:\n",
      "        return words.split()[0]\n",
      "    return False\n",
      "\n",
      "def word_features(words):\n",
      "    '''\n",
      "    Creates features out of all the words in the input text.\n",
      "    '''\n",
      "    words = clean_text(words)\n",
      "    for word in words.split():\n",
      "        yield word\n",
      "\n",
      "def contains_attribution(words):\n",
      "    '''\n",
      "    Returns true if the input string contains a stemmed attribution \n",
      "    word like said.\n",
      "    '''\n",
      "    words = clean_text(words)\n",
      "    contains_attribution = False\n",
      "    for word in words.split():\n",
      "        if word.lower() in ATTRIBUTION_WORDS_STEMMED:\n",
      "            contains_attribution = True\n",
      "            break\n",
      "    return contains_attribution\n",
      "\n",
      "def contains_pronoun(words):\n",
      "    '''\n",
      "    Returns true if the input string contains a pronoun.\n",
      "    '''\n",
      "    words = clean_text(words)\n",
      "    contains_pronoun = False\n",
      "    for word in words.split():\n",
      "        if word.lower() in PRONOUNS:\n",
      "            contains_pronoun = True\n",
      "            break\n",
      "    return contains_pronoun\n",
      "\n",
      "def pct_words_between_quotes(words):\n",
      "    '''\n",
      "    Similar to the num_words_between_quotes feature above but converts\n",
      "    that to a percentage.\n",
      "    '''\n",
      "    total_words = len(words.split())\n",
      "    num_words_between_quotes = 0\n",
      "    lots_of_words_between_quotes = False\n",
      "    for q in bracketed_find(words, '\"', '\"'):\n",
      "        num_words_between_quotes += len(q.split())\n",
      "    return round(float(num_words_between_quotes)/float(total_words), 0)\n",
      "\n",
      "def preceded_by_quote(story, graf_order):\n",
      "    '''\n",
      "    Returns true if the previous paragraph is also a quote.\n",
      "    '''\n",
      "    if Paragraph.objects.get(story=story, order=graf_order-1).quote == True:\n",
      "        return True\n",
      "    return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Citizen Quotes --> DataFrame + Scikit Learn\n",
      "use citizen quotes feature extraction and create pandas dataframe `df_train`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_features(words):\n",
      "    '''\n",
      "    Function that aggregates active features for the maxent classifier and returns\n",
      "    a feature dict in the format expected by NLTK.\n",
      "    '''\n",
      "    features = {} # Start with empty feature dict\n",
      "\n",
      "    # Put features here (found in classify.features)\n",
      "    features['contains_quotes'] = contains_quotes(words)\n",
      "    features['first_quote_index'] = first_quote_index(words)\n",
      "    features['last_word_%s' % last_word(clean_text(words))] = 1\n",
      "    features['said_near_source'] = said_near_source(words)\n",
      "    features['num_words_between_quotes'] = num_words_between_quotes(words)\n",
      "    #for word in words_near_quotes(words):\n",
      "    #    features['%s_near_quote' % word] = 1\n",
      "\n",
      "    return features\n",
      "\n",
      "df_train = pd.DataFrame([get_features(row[2]) for row in tagged_data])\n",
      "df_train = df_train.fillna(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_train.iloc[:5,:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>contains_quotes</th>\n",
        "      <th>first_quote_index</th>\n",
        "      <th>last_word_%13128%</th>\n",
        "      <th>last_word_%13129%</th>\n",
        "      <th>last_word_%32536%</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0</td>\n",
        "      <td>  -1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0</td>\n",
        "      <td>  -1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td>   0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 495</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0</td>\n",
        "      <td>  -1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "   contains_quotes  first_quote_index  last_word_%13128%  last_word_%13129%  \\\n",
        "0                0                 -1                  0                  0   \n",
        "1                0                 -1                  0                  0   \n",
        "2                1                  0                  0                  0   \n",
        "3                1                495                  0                  0   \n",
        "4                0                 -1                  0                  0   \n",
        "\n",
        "   last_word_%32536%  \n",
        "0                  0  \n",
        "1                  0  \n",
        "2                  0  \n",
        "3                  0  \n",
        "4                  0  "
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Cross Validation\n",
      "run 5-fold cross validation using citizen quotes tagged dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = []\n",
      "for train_idx, cv_idx in KFold(n=len(df_train), n_folds=5):\n",
      "    train_cols = filter(lambda x: x != 'contains_quotes', df_train.columns.values.tolist())\n",
      "    \n",
      "    train_x = df_train.loc[train_idx, train_cols]\n",
      "    train_y = df_train.loc[train_idx, 'contains_quotes']\n",
      "    \n",
      "    test_x  = df_train.loc[cv_idx, train_cols]\n",
      "    test_y  = df_train.loc[cv_idx, 'contains_quotes']\n",
      "\n",
      "    lm = LogisticRegression()\n",
      "    lm = lm.fit(train_x, train_y)\n",
      "    score = lm.score(test_x, test_y)\n",
      "\n",
      "    scores.append(score)\n",
      "\n",
      "print \"Score:\", sum(scores)/len(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score: 1.0\n"
       ]
      }
     ],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}