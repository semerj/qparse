{
 "metadata": {
  "name": "",
  "signature": "sha256:567cff87a9c559c1b31ccddc1ff31d8a2e9f1f6e0cc5a69f04ea16988006a3fa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prepare Citizen-Quotes data for QParse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import os\n",
      "from corenlp import corenlp\n",
      "import xmltodict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# import the data we need from citizen quotes\n",
      "paragraphs and whether or not they have quotes (already classified in django app)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn = sqlite3.connect('./quotex.db')\n",
      "conn.row_factory = sqlite3.Row\n",
      "c = conn.cursor()\n",
      "content_paragraph = c.execute(\"SELECT * FROM content_paragraph;\")\n",
      "sqlite_data = []\n",
      "columns=['story_id', 'para_num', 'is_quote', 'text', 'chars']\n",
      "\n",
      "for c in content_paragraph:\n",
      "    sqlite_data.append([c['story_id'], c['order'], c['quote'], c['text'], len(c['text'])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# not all groups of paragraphs are 1-indexed, and soemtimes they skip numbers. must fix.\n",
      "for index, row in enumerate(sqlite_data):\n",
      "    if index == 0:\n",
      "        continue\n",
      "    # same story?\n",
      "    if row[0] == sqlite_data[index-1][0]:\n",
      "        # non-consecutive paragraph numbers?\n",
      "        if row[1] != sqlite_data[index-1][1] + 1:\n",
      "            row[1] = sqlite_data[index-1][1] + 1\n",
      "    # new story? make sure it starts at 1\n",
      "    else:\n",
      "        row[1] = 1\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3,941 labeled paragraphs: 927 w/ quotes and 3,014 w/o quotes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##store this as a pandas DataFrame\n",
      "for easier querying & aggregating"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "storytable = pd.DataFrame(data=sqlite_data, columns = columns)\n",
      "storytable"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>story_id</th>\n",
        "      <th>para_num</th>\n",
        "      <th>is_quote</th>\n",
        "      <th>text</th>\n",
        "      <th>chars</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> The good news started with some wins last year...</td>\n",
        "      <td> 335</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td> Institutional neighbors stepped up, too. The N...</td>\n",
        "      <td> 268</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  3</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"People are beginning to be mobilized in a way...</td>\n",
        "      <td> 138</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  4</td>\n",
        "      <td> 1</td>\n",
        "      <td> At the same time, promising and dedicated gras...</td>\n",
        "      <td> 743</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  5</td>\n",
        "      <td> 0</td>\n",
        "      <td> Inside and outside the bureaucracy, the crisis...</td>\n",
        "      <td> 658</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  6</td>\n",
        "      <td> 0</td>\n",
        "      <td> In the past, some parks neglected collecting f...</td>\n",
        "      <td> 317</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  7</td>\n",
        "      <td> 1</td>\n",
        "      <td> Save the Redwoods League's\u00a0\u00a0executive director...</td>\n",
        "      <td> 259</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  8</td>\n",
        "      <td> 0</td>\n",
        "      <td> In Sonoma, the county parks department and the...</td>\n",
        "      <td> 180</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8   </th>\n",
        "      <td> 13661</td>\n",
        "      <td>  9</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"We'd start by bringing out kids from every si...</td>\n",
        "      <td> 304</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9   </th>\n",
        "      <td> 13661</td>\n",
        "      <td> 10</td>\n",
        "      <td> 0</td>\n",
        "      <td> The Napa County parks department has suggested...</td>\n",
        "      <td> 380</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10  </th>\n",
        "      <td> 13661</td>\n",
        "      <td> 11</td>\n",
        "      <td> 1</td>\n",
        "      <td> The state Legislative Analyst's Office (LAO) a...</td>\n",
        "      <td> 561</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11  </th>\n",
        "      <td> 13661</td>\n",
        "      <td> 12</td>\n",
        "      <td> 0</td>\n",
        "      <td> A few additional millions could be garnered by...</td>\n",
        "      <td> 607</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12  </th>\n",
        "      <td> 13661</td>\n",
        "      <td> 13</td>\n",
        "      <td> 0</td>\n",
        "      <td> This is just a small sampling of the promising...</td>\n",
        "      <td> 371</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13  </th>\n",
        "      <td> 13661</td>\n",
        "      <td> 14</td>\n",
        "      <td> 0</td>\n",
        "      <td> But the shock of closures has also had its ben...</td>\n",
        "      <td> 421</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> Internal Revenue Service and Drug Enforcement ...</td>\n",
        "      <td> 240</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td> Agents also raided Lee's home and the former s...</td>\n",
        "      <td> 161</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "      <td> Separately, federal agents raided the Oakland ...</td>\n",
        "      <td> 305</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0</td>\n",
        "      <td> Agents entered Lee's home with guns early Mond...</td>\n",
        "      <td> 366</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  5</td>\n",
        "      <td> 0</td>\n",
        "      <td> The raids come as Lee's battle with U.S. Attor...</td>\n",
        "      <td> 403</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  6</td>\n",
        "      <td> 0</td>\n",
        "      <td> Barnes declined to offer further comment, sayi...</td>\n",
        "      <td>  78</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  7</td>\n",
        "      <td> 0</td>\n",
        "      <td> Protesters gathered outside Oaksterdam as the ...</td>\n",
        "      <td> 267</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  8</td>\n",
        "      <td> 0</td>\n",
        "      <td> Jason Overman, an aide to Oakland City Council...</td>\n",
        "      <td> 111</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22  </th>\n",
        "      <td> 13662</td>\n",
        "      <td>  9</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"If the federal government has extra resources...</td>\n",
        "      <td> 173</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23  </th>\n",
        "      <td> 13662</td>\n",
        "      <td> 10</td>\n",
        "      <td> 0</td>\n",
        "      <td> Steve DeAngelo, who runs Harborside Health Cen...</td>\n",
        "      <td> 278</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24  </th>\n",
        "      <td> 13662</td>\n",
        "      <td> 11</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"They can't say they're going after a criminal...</td>\n",
        "      <td> 153</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25  </th>\n",
        "      <td> 13663</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> A patchwork of funds \u2014 an $18 million initiati...</td>\n",
        "      <td> 224</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26  </th>\n",
        "      <td> 13663</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td> Nine new school-based health centers providing...</td>\n",
        "      <td> 158</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27  </th>\n",
        "      <td> 13663</td>\n",
        "      <td>  3</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"The greatest challenge of education is the co...</td>\n",
        "      <td> 238</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28  </th>\n",
        "      <td> 13663</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0</td>\n",
        "      <td> That kind of poverty, and the health problems ...</td>\n",
        "      <td> 117</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29  </th>\n",
        "      <td> 13663</td>\n",
        "      <td>  5</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"Teachers know that they cannot teach if kids ...</td>\n",
        "      <td> 214</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3911</th>\n",
        "      <td> 14363</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "      <td>                                                  </td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3912</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> USX Corp. posted a 23% drop in third-quarter p...</td>\n",
        "      <td> 140</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3913</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td> The nation's largest steelmaker earned $175 mi...</td>\n",
        "      <td> 422</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3914</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "      <td> The earnings drop appears particularly steep i...</td>\n",
        "      <td> 338</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3915</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  4</td>\n",
        "      <td> 0</td>\n",
        "      <td> Among segments that continue to operate, thoug...</td>\n",
        "      <td> 455</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3916</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  5</td>\n",
        "      <td> 0</td>\n",
        "      <td> The company attributed lower sales and earning...</td>\n",
        "      <td> 187</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3917</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  6</td>\n",
        "      <td> 0</td>\n",
        "      <td> In the steel division, operating profit droppe...</td>\n",
        "      <td> 431</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3918</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  7</td>\n",
        "      <td> 0</td>\n",
        "      <td> In New York Stock Exchange composite trading y...</td>\n",
        "      <td> 337</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3919</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  8</td>\n",
        "      <td> 0</td>\n",
        "      <td> Charles Bradford, an analyst with Merrill Lync...</td>\n",
        "      <td> 339</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3920</th>\n",
        "      <td> 14364</td>\n",
        "      <td>  9</td>\n",
        "      <td> 0</td>\n",
        "      <td> The energy segment, with a 15% rise in operati...</td>\n",
        "      <td> 420</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3921</th>\n",
        "      <td> 14364</td>\n",
        "      <td> 10</td>\n",
        "      <td> 0</td>\n",
        "      <td> Proceeds of that sale are to be used to reduce...</td>\n",
        "      <td> 327</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3922</th>\n",
        "      <td> 14364</td>\n",
        "      <td> 11</td>\n",
        "      <td> 0</td>\n",
        "      <td> The announced sale of the reserves was followe...</td>\n",
        "      <td> 278</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3923</th>\n",
        "      <td> 14364</td>\n",
        "      <td> 12</td>\n",
        "      <td> 0</td>\n",
        "      <td> Profit for the nine months jumped 21% to $721 ...</td>\n",
        "      <td> 162</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3924</th>\n",
        "      <td> 14364</td>\n",
        "      <td> 13</td>\n",
        "      <td> 0</td>\n",
        "      <td>                                                  </td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3925</th>\n",
        "      <td> 14365</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> John F. Barrett, 40, formerly executive vice p...</td>\n",
        "      <td> 162</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3926</th>\n",
        "      <td> 14365</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td>                                                  </td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3927</th>\n",
        "      <td> 14366</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> Leon J. Level, vice president and chief financ...</td>\n",
        "      <td> 243</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3928</th>\n",
        "      <td> 14366</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td>                                                  </td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3929</th>\n",
        "      <td> 14367</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> David A. DiLoreto, president of metal containe...</td>\n",
        "      <td> 290</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3930</th>\n",
        "      <td> 14367</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td>                                                  </td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3931</th>\n",
        "      <td> 14368</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> Two leading constitutional-law experts said Pr...</td>\n",
        "      <td> 122</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3932</th>\n",
        "      <td> 14368</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td> Professors Philip Kurland of the University of...</td>\n",
        "      <td> 292</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3933</th>\n",
        "      <td> 14368</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "      <td> A line-item veto is a procedure that would all...</td>\n",
        "      <td> 408</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3934</th>\n",
        "      <td> 14368</td>\n",
        "      <td>  4</td>\n",
        "      <td> 1</td>\n",
        "      <td> But the two legal experts, responding to an in...</td>\n",
        "      <td> 329</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3935</th>\n",
        "      <td> 14368</td>\n",
        "      <td>  5</td>\n",
        "      <td> 1</td>\n",
        "      <td> The two professors said the Constitution autho...</td>\n",
        "      <td> 466</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3936</th>\n",
        "      <td> 14368</td>\n",
        "      <td>  6</td>\n",
        "      <td> 0</td>\n",
        "      <td> Sen. Kennedy said in a separate statement that...</td>\n",
        "      <td> 240</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3937</th>\n",
        "      <td> 14368</td>\n",
        "      <td>  7</td>\n",
        "      <td> 0</td>\n",
        "      <td>                                                  </td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3938</th>\n",
        "      <td> 14369</td>\n",
        "      <td>  1</td>\n",
        "      <td> 0</td>\n",
        "      <td> Trinity Industries Inc. said it reached a prel...</td>\n",
        "      <td> 152</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3939</th>\n",
        "      <td> 14369</td>\n",
        "      <td>  2</td>\n",
        "      <td> 0</td>\n",
        "      <td> Trinity said it plans to begin delivery in the...</td>\n",
        "      <td>  75</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3940</th>\n",
        "      <td> 14369</td>\n",
        "      <td>  3</td>\n",
        "      <td> 0</td>\n",
        "      <td>                                                  </td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>3941 rows \u00d7 5 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "      story_id  para_num  is_quote  \\\n",
        "0        13661         1         0   \n",
        "1        13661         2         0   \n",
        "2        13661         3         1   \n",
        "3        13661         4         1   \n",
        "4        13661         5         0   \n",
        "5        13661         6         0   \n",
        "6        13661         7         1   \n",
        "7        13661         8         0   \n",
        "8        13661         9         1   \n",
        "9        13661        10         0   \n",
        "10       13661        11         1   \n",
        "11       13661        12         0   \n",
        "12       13661        13         0   \n",
        "13       13661        14         0   \n",
        "14       13662         1         0   \n",
        "15       13662         2         0   \n",
        "16       13662         3         0   \n",
        "17       13662         4         0   \n",
        "18       13662         5         0   \n",
        "19       13662         6         0   \n",
        "20       13662         7         0   \n",
        "21       13662         8         0   \n",
        "22       13662         9         1   \n",
        "23       13662        10         0   \n",
        "24       13662        11         1   \n",
        "25       13663         1         0   \n",
        "26       13663         2         0   \n",
        "27       13663         3         1   \n",
        "28       13663         4         0   \n",
        "29       13663         5         1   \n",
        "...        ...       ...       ...   \n",
        "3911     14363         3         0   \n",
        "3912     14364         1         0   \n",
        "3913     14364         2         0   \n",
        "3914     14364         3         0   \n",
        "3915     14364         4         0   \n",
        "3916     14364         5         0   \n",
        "3917     14364         6         0   \n",
        "3918     14364         7         0   \n",
        "3919     14364         8         0   \n",
        "3920     14364         9         0   \n",
        "3921     14364        10         0   \n",
        "3922     14364        11         0   \n",
        "3923     14364        12         0   \n",
        "3924     14364        13         0   \n",
        "3925     14365         1         0   \n",
        "3926     14365         2         0   \n",
        "3927     14366         1         0   \n",
        "3928     14366         2         0   \n",
        "3929     14367         1         0   \n",
        "3930     14367         2         0   \n",
        "3931     14368         1         0   \n",
        "3932     14368         2         0   \n",
        "3933     14368         3         0   \n",
        "3934     14368         4         1   \n",
        "3935     14368         5         1   \n",
        "3936     14368         6         0   \n",
        "3937     14368         7         0   \n",
        "3938     14369         1         0   \n",
        "3939     14369         2         0   \n",
        "3940     14369         3         0   \n",
        "\n",
        "                                                   text  chars  \n",
        "0     The good news started with some wins last year...    335  \n",
        "1     Institutional neighbors stepped up, too. The N...    268  \n",
        "2     \"People are beginning to be mobilized in a way...    138  \n",
        "3     At the same time, promising and dedicated gras...    743  \n",
        "4     Inside and outside the bureaucracy, the crisis...    658  \n",
        "5     In the past, some parks neglected collecting f...    317  \n",
        "6     Save the Redwoods League's\u00a0\u00a0executive director...    259  \n",
        "7     In Sonoma, the county parks department and the...    180  \n",
        "8     \"We'd start by bringing out kids from every si...    304  \n",
        "9     The Napa County parks department has suggested...    380  \n",
        "10    The state Legislative Analyst's Office (LAO) a...    561  \n",
        "11    A few additional millions could be garnered by...    607  \n",
        "12    This is just a small sampling of the promising...    371  \n",
        "13    But the shock of closures has also had its ben...    421  \n",
        "14    Internal Revenue Service and Drug Enforcement ...    240  \n",
        "15    Agents also raided Lee's home and the former s...    161  \n",
        "16    Separately, federal agents raided the Oakland ...    305  \n",
        "17    Agents entered Lee's home with guns early Mond...    366  \n",
        "18    The raids come as Lee's battle with U.S. Attor...    403  \n",
        "19    Barnes declined to offer further comment, sayi...     78  \n",
        "20    Protesters gathered outside Oaksterdam as the ...    267  \n",
        "21    Jason Overman, an aide to Oakland City Council...    111  \n",
        "22    \"If the federal government has extra resources...    173  \n",
        "23    Steve DeAngelo, who runs Harborside Health Cen...    278  \n",
        "24    \"They can't say they're going after a criminal...    153  \n",
        "25    A patchwork of funds \u2014 an $18 million initiati...    224  \n",
        "26    Nine new school-based health centers providing...    158  \n",
        "27    \"The greatest challenge of education is the co...    238  \n",
        "28    That kind of poverty, and the health problems ...    117  \n",
        "29    \"Teachers know that they cannot teach if kids ...    214  \n",
        "...                                                 ...    ...  \n",
        "3911                                                         0  \n",
        "3912  USX Corp. posted a 23% drop in third-quarter p...    140  \n",
        "3913  The nation's largest steelmaker earned $175 mi...    422  \n",
        "3914  The earnings drop appears particularly steep i...    338  \n",
        "3915  Among segments that continue to operate, thoug...    455  \n",
        "3916  The company attributed lower sales and earning...    187  \n",
        "3917  In the steel division, operating profit droppe...    431  \n",
        "3918  In New York Stock Exchange composite trading y...    337  \n",
        "3919  Charles Bradford, an analyst with Merrill Lync...    339  \n",
        "3920  The energy segment, with a 15% rise in operati...    420  \n",
        "3921  Proceeds of that sale are to be used to reduce...    327  \n",
        "3922  The announced sale of the reserves was followe...    278  \n",
        "3923  Profit for the nine months jumped 21% to $721 ...    162  \n",
        "3924                                                         0  \n",
        "3925  John F. Barrett, 40, formerly executive vice p...    162  \n",
        "3926                                                         0  \n",
        "3927  Leon J. Level, vice president and chief financ...    243  \n",
        "3928                                                         0  \n",
        "3929  David A. DiLoreto, president of metal containe...    290  \n",
        "3930                                                         0  \n",
        "3931  Two leading constitutional-law experts said Pr...    122  \n",
        "3932  Professors Philip Kurland of the University of...    292  \n",
        "3933  A line-item veto is a procedure that would all...    408  \n",
        "3934  But the two legal experts, responding to an in...    329  \n",
        "3935  The two professors said the Constitution autho...    466  \n",
        "3936  Sen. Kennedy said in a separate statement that...    240  \n",
        "3937                                                         0  \n",
        "3938  Trinity Industries Inc. said it reached a prel...    152  \n",
        "3939  Trinity said it plans to begin delivery in the...     75  \n",
        "3940                                                         0  \n",
        "\n",
        "[3941 rows x 5 columns]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print storytable.query('is_quote != 1 & is_quote != 0')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty DataFrame\n",
        "Columns: [story_id, para_num, is_quote, text, chars]\n",
        "Index: []\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# configure file paths for stanford parser\n",
      "stanpath = 'stanford-coref/stanford-corenlp-full-2014-08-27/'\n",
      "textin = stanpath + 'cq_articles/'\n",
      "xmlout = stanpath + 'cq_xml/'\n",
      "parserpath = stanpath + 'java parser/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grouped = storytable.groupby(['story_id'])\n",
      "for name, group in grouped: \n",
      "    rawstory =  \"\\n\\n\".join(group['text'])\n",
      "    # write each story to a text file for batch processing\n",
      "    with open(textin + str(name) + '.txt', 'w') as outfile: \n",
      "        outfile.write(rawstory.encode('utf8'))\n",
      "    with open(stanpath + 'cq_article_list.txt', 'a') as outfile:\n",
      "        outfile.write('../cq_articles/' + str(name) + '.txt\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run the stanford parser on these stories\n",
      "# parsed = corenlp.batch_parse(textin, parserpath)\n",
      "# for article in parsed:\n",
      "#     print article\n",
      "\n",
      "# doing this on from the CLI, not worth messing around with the settings here to get it to work\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "in `stanford-coref/stanford-corenlp-full-2014-08-27/java parser ` run: \n",
      "\n",
      "`java -cp \"*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -filelist ../cq_article_list.txt -outputDirectory ../cq_xml/ `"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences_out = []\n",
      "document_counter = 0\n",
      "for f in [f for f in os.listdir(xmlout) if f.endswith('.xml')]:\n",
      "    # get original text, and replace corefs with canonical names\n",
      "    article = [g[1] for g in grouped if g[0] == int(f[:-8])][0]\n",
      "    \n",
      "    para_breaks = []\n",
      "    for index, count in enumerate(article['chars']):\n",
      "        if index == 0: \n",
      "            para_breaks.append(0)\n",
      "        else:\n",
      "            # add 2 chars for each paragraph break\n",
      "            para_breaks.append(sum(article['chars'][:index]) + (2 * index))\n",
      "    \n",
      "    with open(xmlout + f, 'r') as xmlfile: \n",
      "        filedict = xmltodict.parse(xmlfile.read())\n",
      "    if 'coreference' in filedict['root']['document']:\n",
      "        corefs =  filedict['root']['document']['coreference']['coreference']\n",
      "        # make sure it's a list, if there's only one coref\n",
      "        if type(corefs) is not list:\n",
      "            corefs = [corefs]\n",
      "    else:\n",
      "        corefs = None\n",
      "\n",
      "    sentences = filedict['root']['document']['sentences']['sentence']\n",
      "    # if only one sentence, it will be a string; back up a level to ensure it's a list\n",
      "    if type(sentences) is not list:\n",
      "        sentences = [sentences]\n",
      "    \n",
      "    paras = []\n",
      "    \n",
      "    # extract a list of persons and their indices in each sentence\n",
      "    names = set()\n",
      "    person_locs = []\n",
      "    for sentence in sentences: \n",
      "        tokens = sentence['tokens']['token']\n",
      "        # if only one token, make it a list\n",
      "        if type(tokens) != list:\n",
      "            tokens = [tokens]\n",
      "        \n",
      "        # create tagged text tuples instead of raw text\n",
      "        tagged_sentence = [(t['word'], t['POS']) for t in tokens]\n",
      "        # add each sentence to the DF, indexed by story and para        \n",
      "        offset = int(tokens[0]['CharacterOffsetBegin']) \n",
      "        # para index is the largest number in para_breaks that is <= offset\n",
      "        para_index = next((para_breaks.index(b)-1 for b in para_breaks if b > offset), len(para_breaks)-1)\n",
      "        paras.append([para_index, tagged_sentence, article.iloc[para_index].loc['is_quote']])\n",
      "        \n",
      "        # aggregate adjacent NER person tags into full names\n",
      "        i = 0\n",
      "        people = {}\n",
      "        while i < len(tokens):\n",
      "            if tokens[i]['NER'] == 'PERSON':\n",
      "                key = i\n",
      "                person = tokens[i]['word']\n",
      "                # keep adding to this person string while the next token is also a person\n",
      "                i += 1\n",
      "                while tokens[i]['NER'] == 'PERSON':\n",
      "                    person += ' ' + tokens[i]['word']\n",
      "                    i += 1\n",
      "                people[key] = person\n",
      "                names.add(person)\n",
      "            i += 1\n",
      "        person_locs.append(people)\n",
      "    \n",
      "    if corefs == None:\n",
      "            # add to data table and move on, no corefs to resolve\n",
      "            \n",
      "            # add to final data structure\n",
      "            for row in paras: \n",
      "                sentences_out.append([int(f[:-8]), row[0], row[1], row[2]])\n",
      "            print \"-\" * 30, document_counter, f\n",
      "            document_counter += 1\n",
      "            continue\n",
      "            \n",
      "    # sometimes it misses last name references to the same person - can we merge them?\n",
      "    # do another pass on person_locs to clean up names\n",
      "    names_to_replace = {}\n",
      "    for name in names: \n",
      "        other_names = {n for n in names if n != name}\n",
      "        # NOTE: this is a naive algorithm, it just picks the first match\n",
      "        # even though multiple people may have the same last name\n",
      "        for n in other_names: \n",
      "            if n in name:\n",
      "                names_to_replace[n] = name\n",
      "    for sent in person_locs:\n",
      "        for loc in sent:\n",
      "            if sent[loc] in names_to_replace:\n",
      "                sent[loc] = names_to_replace[sent[loc]]\n",
      "    \n",
      "    # assemble NERs that correspond to each coref chain\n",
      "    corefs_text = {}\n",
      "    corefs_ends = {}\n",
      "    for coref_chain in corefs:\n",
      "        indices = {}\n",
      "        ends = {}\n",
      "        # weird stuff happens if there's only one coref chain in an article, so back up one level\n",
      "        if coref_chain == \"mention\":\n",
      "            coref_chain = corefs\n",
      "        \n",
      "        for mention in coref_chain['mention']:\n",
      "            sent_index = int(mention['sentence']) - 1\n",
      "            word_start = int(mention['start']) - 1\n",
      "            word_end = int(mention['end']) - 1\n",
      "            if word_start in person_locs[sent_index]:\n",
      "                # does anything in this chain match an NER person?\n",
      "                # IF SO, store whole coref chain,\n",
      "                # but only the names and sentence/word indices\n",
      "                sent_indices = list({int(m['sentence'])-1 for m in coref_chain['mention']})\n",
      "                for sent in sent_indices: \n",
      "                    if sent not in indices:\n",
      "                        indices[sent] = {}\n",
      "                        ends[sent] = {}\n",
      "                    # add words\n",
      "                    words_in_sent = {int(m['start'])-1: '' \n",
      "                                     for m in coref_chain['mention'] \n",
      "                                     if m['sentence'] == str(sent+1)}\n",
      "                    indices[sent].update(words_in_sent)\n",
      "                    # add endings\n",
      "                    ends_in_sent = {int(m['start'])-1: int(m['end'])-1 \n",
      "                                    for m in coref_chain['mention'] \n",
      "                                    if m['sentence'] == str(sent+1)}\n",
      "                    ends[sent].update(ends_in_sent)\n",
      "                break\n",
      "                \n",
      "        if indices:\n",
      "            # get all NER person hits in the chain\n",
      "            # TODO: what if they don't match exactly (eg, coref only catches the last name?)\n",
      "            ners = []\n",
      "            for sent_index in indices: \n",
      "                for word in indices[sent_index]:\n",
      "                    if word in person_locs[sent_index]:\n",
      "                        ners.append(person_locs[sent_index][word])    \n",
      "            # canonical name is the longest one, not necessarily the first\n",
      "            name = max(ners, key=len) \n",
      "            \n",
      "            # replace matched indices with name in corefs_text\n",
      "            for sent_index in indices:\n",
      "                if sent_index not in corefs_text:\n",
      "                    corefs_text[sent_index] = {}\n",
      "                    corefs_ends[sent_index] = {}\n",
      "                for word in indices[sent_index]:\n",
      "                    corefs_text[sent_index][word] = name\n",
      "                    corefs_ends[sent_index].update(ends[sent_index])\n",
      "    \n",
      "    \n",
      "    for i in reversed(sorted(corefs_text.keys())):\n",
      "        # insert any canonical coreference tags\n",
      "        locs = sorted(corefs_text[i].keys())\n",
      "        locs.reverse()\n",
      "        \n",
      "        splice = [(loc, corefs_ends[i][loc]) for loc in locs]\n",
      "        \n",
      "        # account for overlapping coreferences\n",
      "        for index, block in enumerate(splice):\n",
      "            if index + 1 < len(splice):\n",
      "                # detect an overlap in character ranges and keep the one that starts first\n",
      "                if block[0] < splice[index + 1][1]:\n",
      "                    splice[index] = None\n",
      "\n",
      "        for index, loc in enumerate(locs):\n",
      "            # if splice is None, skip it\n",
      "            if splice[index]:\n",
      "                # splice the coref tag into the sentence\n",
      "                end = corefs_ends[i][loc]\n",
      "                name = corefs_text[i][loc]\n",
      "                coref_orig_text = ' '.join(paras[i][1][loc:end])\n",
      "                new_sent = paras[i][1][:loc]\n",
      "                new_sent += [(coref_orig_text + '///' + name, \"COREF\")]\n",
      "                new_sent += paras[i][1][end:]\n",
      "                # make sure it's stored in the original paras list\n",
      "                paras[i][1] = new_sent\n",
      "\n",
      "    # add to final data structure\n",
      "    for row in paras: \n",
      "        sentences_out.append([int(f[:-8]), row[0], row[1], row[2]])\n",
      "    print \"-\" * 30, document_counter, f\n",
      "    document_counter += 1\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'Bothe-Napa', u'NNP'), (u'State', u'NNP'), (u'Park', u'NNP')]\n",
        "[(u'Annadel', u'NNP'), (u'State', u'NNP'), (u'Park', u'NNP')]\n",
        "[(u'Taylor', u'NNP')]\n",
        "[(u'Samuel', u'NNP'), (u'P.', u'NNP'), (u'Taylor', u'NNP'), (u'State', u'NNP'), (u'Park', u'NNP')]\n",
        "[(u'him', u'PRP')]\n",
        "[(u'John', u'NNP'), (u'Muir', u'NNP'), (u\"'s\", u'POS'), (u'31-year-old', u'JJ'), (u'great-great', u'NN'), (u'grandson', u'NN')]\n",
        "[(u'he', u'PRP')]\n",
        "[(u'Hanna', u'NNP')]\n",
        "[(u'I', u'PRP')]\n",
        "[(u'Robert', u'NNP'), (u'Hanna', u'NNP')]\n",
        "[(u'Hendy', u'NNP'), (u'Woods', u'NNP'), (u'State', u'NNP'), (u'Park', u'NNP')]\n",
        "[(u'her', u'PRP$')]\n",
        "[(u'her', u'PRP$')]\n",
        "[(u'a', u'DT'), (u'longtime', u'JJ'), (u'resident', u'NN'), (u'of', u'IN'), (u'Northern', u'NNP'), (u'California', u'NNP'), (u\"'s\", u'POS'), (u'Anderson', u'NNP'), (u'Valley', u'NNP')]\n",
        "[(u'Kathy', u'NNP'), (u'Bailey', u'NNP')]\n",
        "[(u'rescue', u'NN'), (u'Jack', u'NNP'), (u'London', u'NNP'), (u'Historic', u'JJ'), (u'State', u'NNP'), (u'Park', u'NNP')]\n",
        "[(u'a', u'DT'), (u'former', u'JJ'), (u'state', u'NN'), (u'park', u'NN'), (u'superintendent', u'NN')]\n",
        "[(u'Greg', u'NNP'), (u'Hayes', u'NNP')]\n",
        "[(u'Samuel', u'NNP'), (u'P.', u'NNP'), (u'Taylor', u'NNP')]\n",
        "------------------------------ 0 13661.txt.xml\n",
        "[(u'DeAngelo', u'NNP')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'he', u'PRP')]\n",
        "[(u'Lee', u'NNP')]\n",
        "[(u'He', u'PRP')]\n",
        "[(u'his', u'PRP$')]\n",
        "[(u'Steve', u'NNP'), (u'DeAngelo', u'NNP')]\n",
        "[(u'Overman', u'NNP')]\n",
        "[(u'an', u'DT'), (u'aide', u'NN'), (u'to', u'TO'), (u'Oakland', u'NNP'), (u'City', u'NNP'), (u'Councilwoman', u'NNP'), (u'Rebecca', u'NNP'), (u'Kaplan', u'NNP')]\n",
        "[(u'Jason', u'NNP'), (u'Overman', u'NNP')]\n",
        "[(u'Free', u'NNP'), (u'Richard', u'NNP'), (u'Lee', u'NNP')]\n",
        "[(u'Barnes', u'NNP')]\n",
        "[(u'it', u'PRP')]\n",
        "[(u'Lee', u'NNP')]\n",
        "[(u'the', u'DT'), (u'dispensary', u'NN')]\n",
        "[(u'Lee', u'NNP'), (u\"'s\", u'POS'), (u'dispensary', u'NN')]\n",
        "[(u'Haag', u'NNP')]\n",
        "[(u'U.S.', u'NNP'), (u'Attorney', u'NNP'), (u'Melinda', u'NNP'), (u'Haag', u'NNP')]\n",
        "[(u'Lee', u'NNP'), (u\"'s\", u'POS')]\n",
        "[(u'Barnes', u'NNP')]\n",
        "[(u'his', u'PRP$'), (u'home', u'NN')]\n",
        "[(u'Lee', u'NNP')]\n",
        "[(u'Lee', u'NNP'), (u\"'s\", u'POS'), (u'home', u'NN')]\n",
        "[(u'McCormick', u'NNP')]\n",
        "[(u'DEA', u'NNP'), (u'spokeswoman', u'NN'), (u'Joycelyn', u'NNP'), (u'Barnes', u'NNP')]\n",
        "[(u'a', u'DT'), (u'longtime', u'JJ'), (u'medical', u'JJ'), (u'marijuana', u'NN'), (u'activist', u'NN')]\n",
        "[(u'Todd', u'NNP'), (u'McCormick', u'NNP')]\n",
        "[(u'Lee', u'NNP'), (u\"'s\", u'POS')]\n",
        "[(u'Lee', u'NNP'), (u\"'s\", u'POS'), (u'home', u'NN')]\n",
        "------------------------------ 1 13662.txt.xml\n",
        "[(u'Bonnie', u'NNP'), (u'Trinclisti', u'NNP')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'She', u'PRP')]\n",
        "[(u'Clayton', u'NNP')]\n",
        "[(u'Serena', u'NNP'), (u'Clayton', u'NNP')]\n",
        "[(u'Alex', u'NNP'), (u'Briscoe', u'NNP')]\n",
        "[(u'Bonnie', u'NNP'), (u'Trinclisti', u'NNP')]\n",
        "[(u'Clayton', u'NNP')]\n",
        "[(u'Serena', u'NNP'), (u'Clayton', u'NNP')]\n",
        "[(u'Trinclisti', u'NNP')]\n",
        "[(u'She', u'PRP')]\n",
        "[(u'Trinclisti', u'NNP')]\n",
        "[(u'Trinclisti', u'NNP')]\n",
        "[(u'I', u'PRP')]\n",
        "[(u'her', u'PRP$')]\n",
        "[(u'her', u'PRP$')]\n",
        "[(u'She', u'PRP')]\n",
        "[(u'she', u'PRP')]\n",
        "[(u'Nurse', u'NN'), (u'practitioner', u'NN'), (u'Bonnie', u'NNP'), (u'Trinclisti', u'NNP')]\n",
        "[(u'Clayton', u'NNP')]\n",
        "[(u'Clayton', u'NNP')]\n",
        "[(u'Serena', u'NNP'), (u'Clayton', u'NNP')]\n",
        "[(u'Briscoe', u'NNP')]\n",
        "[(u'Clayton', u'NNP')]\n",
        "[(u'You', u'PRP')]\n",
        "[(u'Serena', u'NNP'), (u'Clayton', u'NNP')]\n",
        "[(u'Alex', u'NNP'), (u'Briscoe', u'NNP')]\n",
        "------------------------------ 2 13663.txt.xml\n",
        "[(u'Reiskin', u'NNP')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'Muni', u'JJ')]\n",
        "[(u'Ed', u'NNP'), (u'Reiskin', u'NNP')]\n",
        "[(u'Muni', u'JJ')]\n",
        "[(u'Muni', u'JJ'), (u\"'s\", u'POS')]\n",
        "[(u'president', u'NN'), (u'of', u'IN'), (u'Market', u'NNP'), (u'Street', u'NNP'), (u'Railway', u'NNP'), (u',', u','), (u'a', u'DT'), (u'nonprofit', u'JJ'), (u'group', u'NN'), (u'dedicated', u'VBN'), (u'to', u'TO'), (u'preserving', u'VBG'), (u'Muni', u'JJ'), (u\"'s\", u'POS'), (u'history', u'NN')]\n",
        "[(u'Rick', u'NNP'), (u'Laubscher', u'NNP')]\n",
        "[(u'Muni', u'JJ')]\n",
        "[(u'she', u'PRP')]\n",
        "[(u'Angie', u'NNP'), (u'Murphy', u'NNP'), (u',', u','), (u'a', u'DT'), (u'regular', u'JJ'), (u'Muni', u'JJ'), (u'commuter', u'NN')]\n",
        "[(u'Muni', u'JJ')]\n",
        "[(u'I', u'PRP')]\n",
        "[(u'Muni', u'JJ')]\n",
        "[(u'he', u'PRP')]\n",
        "[(u'a', u'DT'), (u'retired', u'JJ'), (u'Muni', u'JJ'), (u'service', u'NN'), (u'planner', u'NN')]\n",
        "[(u'Peter', u'NNP'), (u'Straus', u'NNP')]\n",
        "[(u'Muni', u'JJ')]\n",
        "------------------------------ 3 13664.txt.xml\n",
        "[(u'he', u'PRP')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'he', u'PRP')]\n",
        "[(u'Damon', u'NNP'), (u'Tevis', u'NNP')]\n",
        "[(u'a', u'DT'), (u'former', u'JJ'), (u'Filipino', u'NNP'), (u'bellboy', u'NN'), (u'credited', u'VBN'), (u'with', u'IN'), (u'marketing', u'VBG'), (u'the', u'DT'), (u'yo-yo', u'NN')]\n",
        "[(u'Pedro', u'NNP'), (u'Flores', u'NNP')]\n",
        "[(u'he', u'PRP')]\n",
        "[(u'his', u'PRP$')]\n",
        "[(u'Nourse', u'NNP')]\n",
        "[(u'ECLECTIC', u'JJ'), (u'EDUCATION', u'NNP'), (u'Joseph', u'NNP'), (u'Nourse', u'NNP')]\n",
        "------------------------------ 4 13665.txt.xml\n",
        "[(u'Katz', u'NNP')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'Katz', u'NNP')]\n",
        "[(u'his', u'PRP$')]\n",
        "[(u'He', u'PRP')]\n",
        "[(u'Murray', u'NNP')]\n",
        "[(u'she', u'PRP')]\n",
        "[(u'Katz', u'NNP')]\n",
        "[(u'her', u'PRP$')]\n",
        "[(u'Katz', u'NNP')]\n",
        "[(u'director', u'NN'), (u'of', u'IN'), (u'community', u'NN'), (u'outreach', u'NN'), (u'for', u'IN'), (u'Pet', u'NNP'), (u'Food', u'NNP'), (u'Express', u'NNP')]\n",
        "[(u'Mike', u'NNP'), (u'Murray', u'NNP')]\n",
        "[(u'Rebecca', u'NNP'), (u'Katz', u'NNP')]\n",
        "------------------------------ 5 13666.txt.xml\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-48922f2ee8fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmlout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxmlfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mfiledict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmltodict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmlfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'coreference'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiledict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcorefs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfiledict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coreference'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coreference'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ian/anaconda/lib/python2.7/site-packages/xmltodict.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(xml_input, encoding, expat, process_namespaces, namespace_separator, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ian/anaconda/lib/python2.7/site-packages/xmltodict.pyc\u001b[0m in \u001b[0;36mstartElement\u001b[0;34m(self, full_name, attrs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstartElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_depth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ian/anaconda/lib/python2.7/site-packages/xmltodict.pyc\u001b[0m in \u001b[0;36m_attrs_to_dict\u001b[0;34m(self, attrs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstartElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ian/anaconda/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m                     \u001b[0;31m# sentinel node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_sents = pd.DataFrame(data=sentences_out, columns=['story_id', 'para_index', 'tagged_sent', 'quote_in_para'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_sents.to_csv('all_sentences_tagged_indexed__complete.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tagged_sents.to_pickle('tagged_sents_dataframe.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# didn't store quote_in_para correctly\n",
      "# tagged_sents = pd.read_csv('all_sentences_tagged_indexed__complete.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print storytable"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for index, row in tagged_sents.iterrows():\n",
      "#     story_id = row['story_id']\n",
      "#     para_index = row['para_index'] + 1\n",
      "#     match = storytable.query('story_id == @story_id & para_num == @para_index')\n",
      "#     if len(match) == 0:\n",
      "#         print story_id, '\\n\\n', para_index, '\\n\\n', row, '\\n\\n', match \n",
      "#         break\n",
      "#     tagged_sents['quote_in_para'][index] = match.iloc[0].loc['is_quote']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tagged_sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tagged_sents.to_csv('all_sentences_tagged_indexed__complete.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}